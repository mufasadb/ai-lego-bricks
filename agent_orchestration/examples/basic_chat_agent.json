{
  "name": "basic_chat_agent",
  "description": "Simple chat agent supporting multiple LLM providers with conversation memory",
  "config": {
    "memory_backend": "auto",
    "default_llm_provider": "gemini",
    "default_model": "gemini-1.5-flash"
  },
  "_alternatives": {
    "anthropic_config": {
      "default_llm_provider": "anthropic",
      "default_model": "claude-3-5-sonnet-20241022"
    },
    "ollama_config": {
      "default_llm_provider": "ollama",
      "default_model": "llama2"
    }
  },
  "steps": [
    {
      "id": "get_user_input",
      "type": "input",
      "description": "Get user query",
      "config": {
        "prompt": "What would you like to know?"
      },
      "outputs": ["user_query"]
    },
    {
      "id": "generate_response",
      "type": "llm_chat",
      "description": "Generate AI response with conversation memory",
      "config": {
        "provider": "gemini",
        "model": "gemini-1.5-flash",
        "use_conversation": true,
        "temperature": 0.7,
        "max_tokens": 1000
      },
      "_alternatives": {
        "anthropic": {
          "provider": "anthropic",
          "model": "claude-3-5-sonnet-20241022"
        },
        "ollama": {
          "provider": "ollama",
          "model": "llama2"
        }
      },
      "inputs": {
        "message": {
          "from_step": "get_user_input",
          "field": "user_query"
        }
      },
      "outputs": ["response"]
    },
    {
      "id": "output_response",
      "type": "output",
      "description": "Return the response",
      "config": {
        "format": "text"
      },
      "inputs": {
        "response": {
          "from_step": "generate_response",
          "field": "response"
        }
      }
    }
  ]
}