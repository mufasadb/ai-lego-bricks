{
  "name": "streaming_agent",
  "description": "Demonstrates streaming capabilities with real-time response and TTS output",
  "config": {
    "default_llm_provider": "ollama",
    "default_model": "gemma3:4b",
    "default_tts_provider": "coqui_xtts"
  },
  "steps": [
    {
      "id": "get_user_input",
      "type": "input",
      "description": "Get user's message",
      "config": {
        "prompt": "What would you like to talk about?"
      },
      "outputs": ["user_message"]
    },
    {
      "id": "stream_response",
      "type": "llm_chat",
      "description": "Generate streaming response",
      "config": {
        "provider": "ollama",
        "model": "gemma3:4b",
        "temperature": 0.7,
        "max_tokens": 500,
        "stream": true,
        "use_conversation": false,
        "system_message": "You are a helpful and engaging conversational assistant. Provide thoughtful, well-structured responses."
      },
      "inputs": {
        "message": {
          "from_step": "get_user_input",
          "field": "user_message"
        }
      },
      "outputs": ["response", "streamed", "chunks"]
    },
    {
      "id": "output_response",
      "type": "output",
      "description": "Output response with streaming and audio info",
      "config": {
        "format": "json"
      },
      "inputs": {
        "text_response": {
          "from_step": "stream_response",
          "field": "response"
        },
        "was_streamed": {
          "from_step": "stream_response",
          "field": "streamed"
        },
        "chunk_count": {
          "from_step": "stream_response",
          "field": "chunks"
        }
      }
    }
  ]
}